{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model from the kuangliu repo \n",
    "from resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Normalization adapted for CIFAR10\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "# Transforms is a list of transformations applied on the 'raw' dataset before the data is fed to the network.\n",
    "# Here, Data augmentation (RandomCrop and Horizontal Flip) are applied to each batch, differently at each epoch, on the training set data only\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "### The data from CIFAR10 are already downloaded in the following folder\n",
    "rootdir = '/opt/img/effdl-cifar10/'\n",
    "\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "\n",
    "trainloader = DataLoader(c10train,batch_size=32,shuffle=True)\n",
    "testloader = DataLoader(c10test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CIFAR10 dataset has 50000 samples\n",
      "Subset of CIFAR10 dataset has 15000 samples\n"
     ]
    }
   ],
   "source": [
    "## number of target samples for the final dataset\n",
    "num_train_examples = len(c10train)\n",
    "num_samples_subset = 15000\n",
    "\n",
    "## We set a seed manually so as to reproduce the results easily\n",
    "seed  = 2147483647\n",
    "\n",
    "## Generate a list of shuffled indices ; with the fixed seed, the permutation will always be the same, for reproducibility\n",
    "indices = list(range(num_train_examples))\n",
    "np.random.RandomState(seed=seed).shuffle(indices)##Â modifies the list in place\n",
    "\n",
    "## We define the Subset using the generated indices\n",
    "c10train_subset = torch.utils.data.Subset(c10train,indices[:num_samples_subset])\n",
    "print(f\"Initial CIFAR10 dataset has {len(c10train)} samples\")\n",
    "print(f\"Subset of CIFAR10 dataset has {len(c10train_subset)} samples\")\n",
    "\n",
    "# Finally we can define anoter dataloader for the training data\n",
    "trainloader_subset = DataLoader(c10train_subset,batch_size=32,shuffle=True)\n",
    "### You can now use either trainloader (full CIFAR10) or trainloader_subset (subset of CIFAR10) to train your networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1. Train a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device conf : using GPU if it is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model used\n",
    "model = ResNet18()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "is_scheduler = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/img/effdl-venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a schedule which gonna divide the lr by 10 when reaching a plateau in performance for the accuracy\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a scheduler to change the learning rate\n",
    "# import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, epochs, is_scheduler):\n",
    "    train_accuracy_list, test_accuracy_list = [], []\n",
    "    train_loss_list, test_loss_list = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Training accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        train_loss_list.append(running_loss / len(trainloader))\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_test += predicted.eq(labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        test_accuracy = 100 * (correct_test / total_test)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        test_loss_list.append(test_loss / len(testloader))\n",
    "\n",
    "        # if using a scheduler\n",
    "        if is_scheduler:\n",
    "            scheduler.step(test_loss / len(testloader))\n",
    "            print(f\"LEARNING RATE UPDATE TO:{optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {running_loss/len(trainloader):.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "              f\"Test Acc: {test_accuracy:.2f}% \",\n",
    "              f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    print(\"Training complete my boss\")\n",
    "\n",
    "    ############# PLOT THE LOSS AND THE ACCURACY PER EPOCH ####################\n",
    "    epochs = range(1, num_epochs+1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4)) \n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, train_loss_list, label=\"Training loss\", color=\"blue\")\n",
    "    axes[0].plot(epochs, test_loss_list, label=\"Testing loss\", color=\"red\")\n",
    "    axes[0].set_title(\"Training and testing loss\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, train_accuracy_list, label=\"Training accuracy\", color=\"blue\")\n",
    "    axes[1].plot(epochs, test_accuracy_list, label=\"Testing accuracy\", color=\"red\")\n",
    "    axes[1].set_title(\"Training and Testing accuracy\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    #return train_loss_list, test_loss_list, train_accuracy_list, test_accuracy_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE UPDATE TO:0.0001\n",
      "Epoch 1: Train Loss: 0.5882, Train Acc: 79.02%, Test Acc: 77.17%  Learning Rate: 0.0001\n",
      "LEARNING RATE UPDATE TO:0.0001\n",
      "Epoch 2: Train Loss: 0.5637, Train Acc: 79.93%, Test Acc: 77.40%  Learning Rate: 0.0001\n",
      "LEARNING RATE UPDATE TO:1e-05\n",
      "Epoch 3: Train Loss: 0.5474, Train Acc: 80.66%, Test Acc: 77.45%  Learning Rate: 1e-05\n",
      "LEARNING RATE UPDATE TO:1e-05\n",
      "Epoch 4: Train Loss: 0.5163, Train Acc: 81.87%, Test Acc: 78.53%  Learning Rate: 1e-05\n",
      "LEARNING RATE UPDATE TO:1e-05\n",
      "Epoch 5: Train Loss: 0.5053, Train Acc: 82.15%, Test Acc: 78.51%  Learning Rate: 1e-05\n",
      "LEARNING RATE UPDATE TO:1.0000000000000002e-06\n",
      "Epoch 6: Train Loss: 0.4972, Train Acc: 82.52%, Test Acc: 78.64%  Learning Rate: 1.0000000000000002e-06\n",
      "LEARNING RATE UPDATE TO:1.0000000000000002e-06\n",
      "Epoch 7: Train Loss: 0.4922, Train Acc: 82.83%, Test Acc: 78.50%  Learning Rate: 1.0000000000000002e-06\n",
      "LEARNING RATE UPDATE TO:1.0000000000000002e-06\n",
      "Epoch 8: Train Loss: 0.4903, Train Acc: 82.51%, Test Acc: 78.55%  Learning Rate: 1.0000000000000002e-06\n",
      "LEARNING RATE UPDATE TO:1.0000000000000002e-07\n",
      "Epoch 9: Train Loss: 0.4974, Train Acc: 82.31%, Test Acc: 78.64%  Learning Rate: 1.0000000000000002e-07\n",
      "LEARNING RATE UPDATE TO:1.0000000000000002e-07\n",
      "Epoch 10: Train Loss: 0.4959, Train Acc: 82.71%, Test Acc: 78.68%  Learning Rate: 1.0000000000000002e-07\n",
      "LEARNING RATE UPDATE TO:1.0000000000000004e-08\n",
      "Epoch 11: Train Loss: 0.4911, Train Acc: 82.37%, Test Acc: 78.59%  Learning Rate: 1.0000000000000004e-08\n",
      "LEARNING RATE UPDATE TO:1.0000000000000004e-08\n",
      "Epoch 12: Train Loss: 0.4923, Train Acc: 82.57%, Test Acc: 78.54%  Learning Rate: 1.0000000000000004e-08\n",
      "LEARNING RATE UPDATE TO:1.0000000000000004e-08\n",
      "Epoch 13: Train Loss: 0.4957, Train Acc: 82.54%, Test Acc: 78.71%  Learning Rate: 1.0000000000000004e-08\n",
      "LEARNING RATE UPDATE TO:1.0000000000000004e-08\n",
      "Epoch 14: Train Loss: 0.4937, Train Acc: 82.57%, Test Acc: 78.71%  Learning Rate: 1.0000000000000004e-08\n",
      "LEARNING RATE UPDATE TO:1.0000000000000004e-08\n",
      "Epoch 15: Train Loss: 0.4928, Train Acc: 82.73%, Test Acc: 78.78%  Learning Rate: 1.0000000000000004e-08\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, trainloader_subset, testloader, epochs=num_epochs, is_scheduler=is_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# check_point = {\n",
    "#     \"epoch\": num_epochs,\n",
    "#     \"model_state\": model.state_dict(),\n",
    "#     \"optimizer\": \"Adam\",\n",
    "#     \"optim_state\": optimizer.state_dict() ,\n",
    "#     \"is_scheduler\": is_scheduler\n",
    "# }\n",
    "# if check_point[\"is_scheduler\"]:\n",
    "#     check_point[\"scheduler\"]=scheduler\n",
    "    \n",
    "# torch.save(check_point, './model_state/resnet18_model_simple.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We load the dictionary\n",
    "# loaded_cpt = torch.load('./model_state/resnet18_model_simple.pth')\n",
    "\n",
    "# new_model = ResNet18()\n",
    "# new_model.load_state_dict(loaded_cpt['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "\n",
    "# 1) Presentation of the assigned paper (ResNet)\n",
    "\n",
    "# 2) Hyperparameter exploration strategy (C10 full)\n",
    "\n",
    "#   a) Architecture HP\n",
    "#       - Type (ResNet, DenseNet, etc.)\n",
    "#       - Depth (nb of layers)\n",
    "#       - Width (nb of filter per layer, growthrate, etc.)\n",
    "\n",
    "#   b) Training\n",
    "#       - Optimizer (SGD, Adam, etc.)\n",
    "#       - Learning rate, scheduler\n",
    "#       - Nb of epochs\n",
    "#       - Data augmentation (to struggle overfitting)\n",
    "#       - batch size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effdl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

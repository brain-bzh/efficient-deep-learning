{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model from the kuangliu repo \n",
    "from resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import MixUp\n",
    "\n",
    "mixup_transform = MixUp(num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "is_scheduler = True\n",
    "weight = 0.0005\n",
    "is_mixup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalization adapted for CIFAR10\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "# Transforms is a list of transformations applied on the 'raw' dataset before the data is fed to the network.\n",
    "# Here, Data augmentation (RandomCrop and Horizontal Flip) are applied to each batch, differently at each epoch, on the training set data only\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "### The data from CIFAR10 are already downloaded in the following folder\n",
    "rootdir = '/opt/img/effdl-cifar10/'\n",
    "\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "\n",
    "trainloader = DataLoader(c10train,batch_size=batch_size,shuffle=True)\n",
    "testloader = DataLoader(c10test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of target samples for the final dataset\n",
    "num_train_examples = len(c10train)\n",
    "num_samples_subset = 15000\n",
    "\n",
    "## We set a seed manually so as to reproduce the results easily\n",
    "seed  = 2147483647\n",
    "\n",
    "## Generate a list of shuffled indices ; with the fixed seed, the permutation will always be the same, for reproducibility\n",
    "indices = list(range(num_train_examples))\n",
    "np.random.RandomState(seed=seed).shuffle(indices)## modifies the list in place\n",
    "\n",
    "## We define the Subset using the generated indices\n",
    "c10train_subset = torch.utils.data.Subset(c10train,indices[:num_samples_subset])\n",
    "print(f\"Initial CIFAR10 dataset has {len(c10train)} samples\")\n",
    "print(f\"Subset of CIFAR10 dataset has {len(c10train_subset)} samples\")\n",
    "\n",
    "# Finally we can define anoter dataloader for the training data\n",
    "trainloader_subset = DataLoader(c10train_subset,batch_size=batch_size,shuffle=True)\n",
    "### You can now use either trainloader (full CIFAR10) or trainloader_subset (subset of CIFAR10) to train your networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1. Train a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device conf : using GPU if it is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model used\n",
    "model = ResNet18()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a schedule which gonna divide the lr by 10 when reaching a plateau in performance for the accuracy\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a scheduler to change the learning rate\n",
    "# import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, epochs, is_scheduler):\n",
    "    train_accuracy_list, test_accuracy_list = [], []\n",
    "    train_loss_list, test_loss_list = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Training accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        train_loss_list.append(running_loss / len(trainloader))\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_test += predicted.eq(labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        test_accuracy = 100 * (correct_test / total_test)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        test_loss_list.append(test_loss / len(testloader))\n",
    "\n",
    "        # if using a scheduler\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if is_scheduler:\n",
    "            scheduler.step(test_loss / len(testloader))\n",
    "            if current_lr != optimizer.param_groups[0]['lr']:\n",
    "                print(f\"LEARNING RATE UPDATE TO:{optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {running_loss/len(trainloader):.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "              f\"Test Acc: {test_accuracy:.2f}% \",\n",
    "              f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    print(\"Training complete my boss\")\n",
    "\n",
    "    ############# PLOT THE LOSS AND THE ACCURACY PER EPOCH ####################\n",
    "    epochs = range(1, num_epochs+1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4)) \n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, train_loss_list, label=\"Training loss\", color=\"blue\")\n",
    "    axes[0].plot(epochs, test_loss_list, label=\"Testing loss\", color=\"red\")\n",
    "    axes[0].set_title(\"Training and testing loss\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, train_accuracy_list, label=\"Training accuracy\", color=\"blue\")\n",
    "    axes[1].plot(epochs, test_accuracy_list, label=\"Testing accuracy\", color=\"red\")\n",
    "    axes[1].set_title(\"Training and Testing accuracy\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    #return train_loss_list, test_loss_list, train_accuracy_list, test_accuracy_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_model_bis(model, trainloader, testloader, epochs, is_scheduler, is_mixup, save_dir=\"output\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure save directory exists\n",
    "    log_path = os.path.join(save_dir, \"training_logs.txt\")\n",
    "    \n",
    "    train_accuracy_list, test_accuracy_list = [], []\n",
    "    train_loss_list, test_loss_list = [], []\n",
    "\n",
    "    with open(log_path, \"w\") as f:  # Open log file in write mode\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            for images, labels in trainloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                if is_mixup:\n",
    "                    images, labels = mixup_transform(images, labels)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Training accuracy\n",
    "                _, predicted = outputs.max(1)\n",
    "\n",
    "                if is_mixup:\n",
    "                    labels = labels.argmax(dim=1)\n",
    "                    \n",
    "                correct_train += predicted.eq(labels).sum().item()\n",
    "                total_train += labels.size(0)\n",
    "\n",
    "            train_accuracy = 100 * correct_train / total_train\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "            train_loss_list.append(running_loss / len(trainloader))\n",
    "\n",
    "            # Evaluate on test set\n",
    "            model.eval()\n",
    "            correct_test = 0\n",
    "            total_test = 0\n",
    "            test_loss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in testloader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    test_loss += loss.item()\n",
    "\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    correct_test += predicted.eq(labels).sum().item()\n",
    "                    total_test += labels.size(0)\n",
    "\n",
    "            test_accuracy = 100 * (correct_test / total_test)\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "            test_loss_list.append(test_loss / len(testloader))\n",
    "\n",
    "            # If using a scheduler\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            if is_scheduler:\n",
    "                scheduler.step(test_loss / len(testloader))\n",
    "                if current_lr != optimizer.param_groups[0]['lr']:\n",
    "                    print(f\"LEARNING RATE UPDATE TO:{optimizer.param_groups[0]['lr']}\")\n",
    "                    f.write(f\"LEARNING RATE UPDATE TO:{optimizer.param_groups[0]['lr']}\\n\")\n",
    "\n",
    "            # Logging epoch results\n",
    "            epoch_log = (f\"Epoch {epoch+1}: Train Loss: {running_loss/len(trainloader):.4f}, \"\n",
    "                         f\"Train Acc: {train_accuracy:.2f}%, Test Acc: {test_accuracy:.2f}% \"\n",
    "                         f\"Learning Rate: {optimizer.param_groups[0]['lr']}\\n\")\n",
    "\n",
    "            print(epoch_log.strip())  # Print in the notebook\n",
    "            f.write(epoch_log)  # Save to log file\n",
    "\n",
    "    print(\"Training complete my boss\")\n",
    "    \n",
    "    ####### PLOT THE LOSS AND ACCURACY PER EPOCH AND SAVE ########\n",
    "    epochs_range = range(1, epochs+1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5)) \n",
    "\n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs_range, train_loss_list, label=\"Training loss\", color=\"blue\")\n",
    "    axes[0].plot(epochs_range, test_loss_list, label=\"Testing loss\", color=\"red\")\n",
    "    axes[0].set_title(\"Training and Testing Loss\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs_range, train_accuracy_list, label=\"Training accuracy\", color=\"blue\")\n",
    "    axes[1].plot(epochs_range, test_accuracy_list, label=\"Testing accuracy\", color=\"red\")\n",
    "    axes[1].set_title(\"Training and Testing Accuracy\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(save_dir, \"training_plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Training logs saved to: {log_path}\")\n",
    "    print(f\"Training plot saved to: {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "#train_model_bis(model, trainloader, testloader, epochs=num_epochs, is_scheduler=is_scheduler, is_mixup=is_mixup, save_dir=\"output\")\n",
    "train_model(model, trainloader, testloader, epochs=num_epochs, is_scheduler=is_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# check_point = {\n",
    "#     \"epoch\": num_epochs,\n",
    "#     \"model_state\": model.state_dict(),\n",
    "#     \"optimizer\": \"Adam\",\n",
    "#     \"optim_state\": optimizer.state_dict() ,\n",
    "#     \"is_scheduler\": is_scheduler\n",
    "# }\n",
    "# if check_point[\"is_scheduler\"]:\n",
    "#     check_point[\"scheduler\"]=scheduler\n",
    "    \n",
    "# torch.save(check_point, './model_state/resnet18_model_simple.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We load the dictionary\n",
    "# loaded_cpt = torch.load('./model_state/resnet18_model_simple.pth')\n",
    "\n",
    "# new_model = ResNet18()\n",
    "# new_model.load_state_dict(loaded_cpt['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "\n",
    "# 1) Presentation of the assigned paper (ResNet)\n",
    "\n",
    "# 2) Hyperparameter exploration strategy (C10 full)\n",
    "\n",
    "#   a) Architecture HP\n",
    "#       - Type (ResNet, DenseNet, etc.)\n",
    "#       - Depth (nb of layers)\n",
    "#       - Width (nb of filter per layer, growthrate, etc.)\n",
    "\n",
    "#   b) Training\n",
    "#       - Optimizer (SGD, Adam, etc.)\n",
    "#       - Learning rate, scheduler\n",
    "#       - Nb of epochs\n",
    "#       - Regularization ( weight decay)\n",
    "#       - Data augmentation (to struggle overfitting)\n",
    "#       - batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, we are underfitting, we have to overfit first then fight overfitting.\n",
    "# I've increased the batch size and as a result, the accuracy and loss increased and decreased respectively. \n",
    "# However, the model is still not overfitting, why? -> No generalization power? Model capacity limitation? -> more layer? change the achitecture?\n",
    "# --> use the full dataset to see if the problem is persisting \n",
    "\n",
    "# Do we have to remove the transform (DA) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print head dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effdl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

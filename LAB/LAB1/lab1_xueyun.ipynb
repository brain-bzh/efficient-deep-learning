{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model from the kuangliu repo \n",
    "from resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "is_scheduler = True\n",
    "weight = 0.0005\n",
    "is_mixup = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "## Normalization adapted for CIFAR10\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "# Transforms is a list of transformations applied on the 'raw' dataset before the data is fed to the network.\n",
    "# Here, Data augmentation (RandomCrop and Horizontal Flip) are applied to each batch, differently at each epoch, on the training set data only\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "\n",
    "### The data from CIFAR10 are already downloaded in the following folder\n",
    "rootdir = '/opt/img/effdl-cifar10/'\n",
    "\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "\n",
    "trainloader = DataLoader(c10train,batch_size=batch_size,shuffle=True)\n",
    "testloader = DataLoader(c10test,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial CIFAR10 dataset has 50000 samples\n",
      "Subset of CIFAR10 dataset has 15000 samples\n"
     ]
    }
   ],
   "source": [
    "## number of target samples for the final dataset\n",
    "num_train_examples = len(c10train)\n",
    "num_samples_subset = 15000\n",
    "\n",
    "## We set a seed manually so as to reproduce the results easily\n",
    "seed  = 2147483647\n",
    "\n",
    "## Generate a list of shuffled indices ; with the fixed seed, the permutation will always be the same, for reproducibility\n",
    "indices = list(range(num_train_examples))\n",
    "np.random.RandomState(seed=seed).shuffle(indices)## modifies the list in place\n",
    "\n",
    "## We define the Subset using the generated indices\n",
    "c10train_subset = torch.utils.data.Subset(c10train,indices[:num_samples_subset])\n",
    "print(f\"Initial CIFAR10 dataset has {len(c10train)} samples\")\n",
    "print(f\"Subset of CIFAR10 dataset has {len(c10train_subset)} samples\")\n",
    "\n",
    "# Finally we can define anoter dataloader for the training data\n",
    "trainloader_subset = DataLoader(c10train_subset,batch_size=batch_size,shuffle=True)\n",
    "### You can now use either trainloader (full CIFAR10) or trainloader_subset (subset of CIFAR10) to train your networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1. Train a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device conf : using GPU if it is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model used\n",
    "model = ResNet18()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/img/effdl-venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a schedule which gonna divide the lr by 10 when reaching a plateau in performance for the accuracy\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a scheduler to change the learning rate\n",
    "# import librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, epochs, is_scheduler):\n",
    "    train_accuracy_list, test_accuracy_list = [], []\n",
    "    train_loss_list, test_loss_list = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Training accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct_train += predicted.eq(labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        train_loss_list.append(running_loss / len(trainloader))\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_test += predicted.eq(labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        test_accuracy = 100 * (correct_test / total_test)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        test_loss_list.append(test_loss / len(testloader))\n",
    "\n",
    "        # if using a scheduler\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if is_scheduler:\n",
    "            scheduler.step(test_loss / len(testloader))\n",
    "            if current_lr != optimizer.param_groups[0]['lr']:\n",
    "                print(f\"LEARNING RATE UPDATE TO:{optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {running_loss/len(trainloader):.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "              f\"Test Acc: {test_accuracy:.2f}% \",\n",
    "              f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    print(\"Training complete my boss\")\n",
    "\n",
    "    ############# PLOT THE LOSS AND THE ACCURACY PER EPOCH ####################\n",
    "    epochs = range(1, num_epochs+1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4)) \n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, train_loss_list, label=\"Training loss\", color=\"blue\")\n",
    "    axes[0].plot(epochs, test_loss_list, label=\"Testing loss\", color=\"red\")\n",
    "    axes[0].set_title(\"Training and testing loss\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, train_accuracy_list, label=\"Training accuracy\", color=\"blue\")\n",
    "    axes[1].plot(epochs, test_accuracy_list, label=\"Testing accuracy\", color=\"red\")\n",
    "    axes[1].set_title(\"Training and Testing accuracy\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    #return train_loss_list, test_loss_list, train_accuracy_list, test_accuracy_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## MixUp Useful Functions\n",
    "def mixup_data(x, y, alpha=1):\n",
    "    \"\"\"Returns mixed inputs, pairs of targets, and lambda\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = float(np.random.beta(alpha, alpha))  # Ensure float type\n",
    "    else:\n",
    "        lam = 1.0\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size, device=x.device)  # Uses the same device as x\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Computes the mixup loss\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_bis(model, trainloader, testloader, epochs, is_scheduler, is_mixup):\n",
    "    train_accuracy_list, test_accuracy_list = [], []\n",
    "    train_loss_list, test_loss_list = [], []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        if is_mixup:\n",
    "            for images, labels in trainloader:\n",
    "                images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=1.0)\n",
    "                images, labels_a, labels_b = images.to(device), labels_a.to(device), labels_b.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                # Training accuracy\n",
    "                _, predicted = outputs.max(1)\n",
    "\n",
    "                correct_train += (lam * predicted.eq(labels_a).sum().item() + (1 - lam) * predicted.eq(labels_b).sum().item())\n",
    "                total_train += labels_a.size(0)\n",
    "\n",
    "        else:\n",
    "            for images, labels in trainloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Training accuracy\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_train += predicted.eq(labels).sum().item()\n",
    "                total_train += labels.size(0)\n",
    "\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        train_loss_list.append(running_loss / len(trainloader))\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in testloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct_test += predicted.eq(labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        test_accuracy = 100 * (correct_test / total_test)\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        test_loss_list.append(test_loss / len(testloader))\n",
    "\n",
    "        # if using a scheduler\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        if is_scheduler:\n",
    "            scheduler.step(test_loss / len(testloader))\n",
    "            if current_lr != optimizer.param_groups[0]['lr']:\n",
    "                print(f\"LEARNING RATE UPDATE TO:{optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: \"\n",
    "              f\"Train Loss: {running_loss/len(trainloader):.4f}, \"\n",
    "              f\"Train Acc: {train_accuracy:.2f}%, \"\n",
    "              f\"Test Acc: {test_accuracy:.2f}% \",\n",
    "              f\"Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "    print(\"Training complete my boss\")\n",
    "\n",
    "    ############# PLOT THE LOSS AND THE ACCURACY PER EPOCH ####################\n",
    "    epochs = range(1, num_epochs+1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4)) \n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(epochs, train_loss_list, label=\"Training loss\", color=\"blue\")\n",
    "    axes[0].plot(epochs, test_loss_list, label=\"Testing loss\", color=\"red\")\n",
    "    axes[0].set_title(\"Training and testing loss\")\n",
    "    axes[0].set_xlabel(\"Epochs\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot(epochs, train_accuracy_list, label=\"Training accuracy\", color=\"blue\")\n",
    "    axes[1].plot(epochs, test_accuracy_list, label=\"Testing accuracy\", color=\"red\")\n",
    "    axes[1].set_title(\"Training and Testing accuracy\")\n",
    "    axes[1].set_xlabel(\"Epochs\")\n",
    "    axes[1].set_ylabel(\"Accuracy (%)\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "    #return train_loss_list, test_loss_list, train_accuracy_list, test_accuracy_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.4656, Train Acc: 46.20%, Test Acc: 54.84%  Learning Rate: 0.001\n",
      "Epoch 2: Train Loss: 1.0295, Train Acc: 63.40%, Test Acc: 57.93%  Learning Rate: 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#train_model_bis(model, trainloader, testloader, epochs=num_epochs, is_scheduler=is_scheduler, is_mixup=is_mixup, save_dir=\"output\")\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_model_bis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_mixup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_mixup\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mtrain_model_bis\u001b[0;34m(model, trainloader, testloader, epochs, is_scheduler, is_mixup)\u001b[0m\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Training accuracy\u001b[39;00m\n\u001b[1;32m     38\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "#train_model_bis(model, trainloader, testloader, epochs=num_epochs, is_scheduler=is_scheduler, is_mixup=is_mixup, save_dir=\"output\")\n",
    "train_model_bis(model, trainloader, testloader, epochs=num_epochs, is_scheduler=is_scheduler, is_mixup=is_mixup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Save the model\n",
    "# check_point = {\n",
    "#     \"epoch\": num_epochs,\n",
    "#     \"model_state\": model.state_dict(),\n",
    "#     \"optimizer\": \"Adam\",\n",
    "#     \"optim_state\": optimizer.state_dict() ,\n",
    "#     \"is_weight\": weight ,\n",
    "#     \"is_scheduler\": is_scheduler\n",
    "# }\n",
    "# if check_point[\"is_scheduler\"]:\n",
    "#     check_point[\"scheduler\"]=scheduler\n",
    "    \n",
    "# torch.save(check_point, './model_state/resnet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We load the dictionary\n",
    "# loaded_cpt = torch.load('./model_state/resnet18_model_simple.pth')\n",
    "\n",
    "# new_model = ResNet18()\n",
    "# new_model.load_state_dict(loaded_cpt['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "\n",
    "# 1) Presentation of the assigned paper (ResNet)\n",
    "\n",
    "# 2) Hyperparameter exploration strategy (C10 full)\n",
    "\n",
    "#   a) Architecture HP\n",
    "#       - Type (ResNet, DenseNet, etc.)\n",
    "#       - Depth (nb of layers)\n",
    "#       - Width (nb of filter per layer, growthrate, etc.)\n",
    "\n",
    "#   b) Training\n",
    "#       - Optimizer (SGD, Adam, etc.)\n",
    "#       - Learning rate, scheduler\n",
    "#       - Nb of epochs\n",
    "#       - Regularization ( weight decay)\n",
    "#       - Data augmentation (to struggle overfitting)\n",
    "#       - batch size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, we are underfitting, we have to overfit first then fight overfitting.\n",
    "# I've increased the batch size and as a result, the accuracy and loss increased and decreased respectively. \n",
    "# However, the model is still not overfitting, why? -> No generalization power? Model capacity limitation? -> more layer? change the achitecture?\n",
    "# --> use the full dataset to see if the problem is persisting \n",
    "\n",
    "# Do we have to remove the transform (DA) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print head dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effdl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

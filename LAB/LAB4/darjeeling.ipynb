{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leami\\AppData\\Local\\Temp\\ipykernel_11700\\643435494.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./pruned_model.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle chargé en half precision !\n",
      "Nombre total de paramètres: 11,173,962\n",
      "Nombre total d'opérations (FLOPs): 1,110,864,660\n",
      "Score calculé: 1.1340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leami\\AppData\\Local\\Temp\\ipykernel_11700\\643435494.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./pruned_model.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from torchinfo import summary\n",
    "\n",
    "# Ajouter le chemin pour importer ResNet18\n",
    "sys.path.append(os.path.abspath(\"../LAB1\"))\n",
    "from resnet import ResNet18\n",
    "\n",
    "def calculate_score(structured_pruning=0.2, global_pruning=0.2, is_half_precision=True):\n",
    "    # Créer le modèle\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ResNet18().to(device)\n",
    "    checkpoint = torch.load(\"./pruned_model.pth\", map_location=device)\n",
    "    checkpoint = torch.load(\"./pruned_model.pth\", map_location=device)\n",
    "    model.load_state_dict(checkpoint, strict=False) \n",
    "    print(\"Modèle chargé en half precision !\")\n",
    "    \n",
    "    # Afficher les statistiques du modèle\n",
    "    stats = summary(model, (1, 3, 32, 32), verbose=0)\n",
    "    \n",
    "    total_params = stats.total_params\n",
    "    total_macs = stats.total_mult_adds  # Nombre d'opérations MAC (approximation des FLOPs)\n",
    "    \n",
    "    # Convertir MAC en FLOPs (environ 2 FLOPs par MAC)\n",
    "    total_flops = total_macs * 2\n",
    "    \n",
    "    # Calculer le score\n",
    "    p_s = structured_pruning\n",
    "    p_u = global_pruning\n",
    "    q_w = 16 if is_half_precision else 32  # 16 bits pour half precision, 32 pour full\n",
    "    \n",
    "    # On suppose q_a = 32 bits pour les activations\n",
    "    max_q = max(q_w, 16)\n",
    "    \n",
    "    w = total_params\n",
    "    f = total_flops\n",
    "    \n",
    "    param_term = (1 - (p_s + p_u)) * (q_w/32) * w / (5.6e6)\n",
    "    ops_term = (1 - p_s) * (max_q/32) * f / (8.3e8)\n",
    "    \n",
    "    score = param_term + ops_term\n",
    "    \n",
    "    print(f\"Nombre total de paramètres: {total_params:,}\")\n",
    "    print(f\"Nombre total d'opérations (FLOPs): {total_flops:,}\")\n",
    "    print(f\"Score calculé: {score:.4f}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Calculer le score avec vos paramètres\n",
    "score = calculate_score(structured_pruning=0.2, global_pruning=0.2, is_half_precision=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight_orig', 'conv1.weight_mask', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight_orig', 'layer1.0.conv1.weight_mask', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight_orig', 'layer1.0.conv2.weight_mask', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight_orig', 'layer1.1.conv1.weight_mask', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight_orig', 'layer1.1.conv2.weight_mask', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight_orig', 'layer2.0.conv1.weight_mask', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight_orig', 'layer2.0.conv2.weight_mask', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.shortcut.0.weight_orig', 'layer2.0.shortcut.0.weight_mask', 'layer2.0.shortcut.1.weight', 'layer2.0.shortcut.1.bias', 'layer2.0.shortcut.1.running_mean', 'layer2.0.shortcut.1.running_var', 'layer2.0.shortcut.1.num_batches_tracked', 'layer2.1.conv1.weight_orig', 'layer2.1.conv1.weight_mask', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight_orig', 'layer2.1.conv2.weight_mask', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight_orig', 'layer3.0.conv1.weight_mask', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight_orig', 'layer3.0.conv2.weight_mask', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.shortcut.0.weight_orig', 'layer3.0.shortcut.0.weight_mask', 'layer3.0.shortcut.1.weight', 'layer3.0.shortcut.1.bias', 'layer3.0.shortcut.1.running_mean', 'layer3.0.shortcut.1.running_var', 'layer3.0.shortcut.1.num_batches_tracked', 'layer3.1.conv1.weight_orig', 'layer3.1.conv1.weight_mask', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight_orig', 'layer3.1.conv2.weight_mask', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight_orig', 'layer4.0.conv1.weight_mask', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight_orig', 'layer4.0.conv2.weight_mask', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.shortcut.0.weight_orig', 'layer4.0.shortcut.0.weight_mask', 'layer4.0.shortcut.1.weight', 'layer4.0.shortcut.1.bias', 'layer4.0.shortcut.1.running_mean', 'layer4.0.shortcut.1.running_var', 'layer4.0.shortcut.1.num_batches_tracked', 'layer4.1.conv1.weight_orig', 'layer4.1.conv1.weight_mask', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight_orig', 'layer4.1.conv2.weight_mask', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'linear.bias', 'linear.weight_orig', 'linear.weight_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leami\\AppData\\Local\\Temp\\ipykernel_11700\\3176181146.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"./pruned_model.pth\", map_location=device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(\"./pruned_model.pth\", map_location=device)\n",
    "print(checkpoint.keys())  # Vérifie les clés contenues dans le fichier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modele de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres: 11,173,962\n",
      "Nombre total d'opérations (FLOPs): 1,110,864,660\n",
      "Score calculé: 3.3337\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from torchinfo import summary\n",
    "\n",
    "# Ajouter le chemin pour importer ResNet18\n",
    "sys.path.append(os.path.abspath(\"../LAB1\"))\n",
    "from resnet import ResNet18\n",
    "\n",
    "def calculate_score(structured_pruning=0.2, global_pruning=0.2, is_half_precision=True):\n",
    "    # Créer le modèle\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = ResNet18().to(device)\n",
    "    \n",
    "    # Afficher les statistiques du modèle\n",
    "    stats = summary(model, (1, 3, 32, 32), verbose=0)\n",
    "    \n",
    "    total_params = stats.total_params\n",
    "    total_macs = stats.total_mult_adds  # Nombre d'opérations MAC (approximation des FLOPs)\n",
    "    \n",
    "    # Convertir MAC en FLOPs (environ 2 FLOPs par MAC)\n",
    "    total_flops = total_macs * 2\n",
    "    \n",
    "    # Calculer le score\n",
    "    p_s = structured_pruning\n",
    "    p_u = global_pruning\n",
    "    q_w = 16 if is_half_precision else 32  # 16 bits pour half precision, 32 pour full\n",
    "    \n",
    "    # On suppose q_a = 32 bits pour les activations\n",
    "    max_q = max(q_w, 32)\n",
    "    \n",
    "    w = total_params\n",
    "    f = total_flops\n",
    "    \n",
    "    param_term = (1 - (p_s + p_u)) * (q_w/32) * w / (5.6e6)\n",
    "    ops_term = (1 - p_s) * (max_q/32) * f / (8.3e8)\n",
    "    \n",
    "    score = param_term + ops_term\n",
    "    \n",
    "    print(f\"Nombre total de paramètres: {total_params:,}\")\n",
    "    print(f\"Nombre total d'opérations (FLOPs): {total_flops:,}\")\n",
    "    print(f\"Score calculé: {score:.4f}\")\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Calculer le score avec vos paramètres\n",
    "score = calculate_score(structured_pruning=0, global_pruning=0, is_half_precision=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
